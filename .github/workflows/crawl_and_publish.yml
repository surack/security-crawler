name: Crawl and Publish Reports

on:
  workflow_dispatch:
    inputs:
      url:
        description: 'Target website URL to crawl (must be your own site!)'
        required: true
        default: 'https://example.com'

jobs:
  crawl:
    runs-on: ubuntu-latest

    permissions:
      contents: write   # Needed to push results into repo

    steps:
      # 1. Checkout repo
      - name: Checkout repository
        uses: actions/checkout@v3

      # 2. Setup Python
      - name: Set up Python
        uses: actions/setup-python@v4
        with:
          python-version: '3.10'

      # 3. Install dependencies
      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install requests beautifulsoup4 tldextract pandas lxml

      # 4. Run crawler (⚠️ updated filename!)
      - name: Run crawler
        run: |
          ls -lah
          echo "Running crawler..."
          python passive_security_crawler.py --url "${{ github.event.inputs.url }}" --max-pages 50 --delay 1 --respect-robots true --out-prefix report

      # 5. Commit and push reports into "reports" branch
      - name: Commit and push reports
        run: |
          git config --global user.name "github-actions[bot]"
          git config --global user.email "github-actions[bot]@users.noreply.github.com"

          git fetch origin
          git checkout -B reports

          git add report.json report.csv || echo "No reports to add"
          git commit -m "Add crawler reports for ${{ github.event.inputs.url }} [skip ci]" || echo "No changes to commit"
          git push origin reports
        env:
          GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}
